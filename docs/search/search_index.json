{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview Intel\u00ae Robot SDK Project contains robotics related open source software components under ROS2 framework for realsense based perceptual computation, neuron network based object and people face detection, object tracking and 3D localization, SLAM and navigation. Package Included ros2_intel_realsense ros2_intel_realsense for using Intel\u00ae RealSense\u2122 cameras (D400 series) with ROS2. ros2_object_analytics ros2_object_analytics is a group of ROS2 packages for real-time object detection, localization and tracking. These packages aim to provide real-time object analyses over RGB-D camera inputs, enabling ROS developer to easily create amazing robotics advanced features, like intelligent collision avoidance and semantic SLAM. ros2_openvino_toolkit The OpenVINO\u2122 toolkit quickly deploys applications and solutions that emulate human vision. Based on Convolutional Neural Networks (CNN), the toolkit extends computer vision (CV) workloads across Intel\u00ae hardware, maximizing performance. This project is a ROS2 wrapper for CV API of OpenVINO\u2122, providing the following features: \u2002\u2002 * Support CPU and GPU platforms \u2002\u2002 * Support standard USB camera and Intel\u00ae RealSense\u2122 camera \u2002\u2002 * Support Video or Image file as detection source \u2002\u2002 * Face detection: \u2002\u2002\u2002\u2002\u2002\u2002 Emotion recognition \u2002\u2002\u2002\u2002\u2002\u2002 Age and gender recognition \u2002\u2002\u2002\u2002\u2002\u2002 Head pose recognition \u2002\u2002 * Object detection \u2002\u2002 * Object segmentation \u2002\u2002 * Demo application to show above detection and recognitions navigation2 The ROS 2 Navigation System is the control system that enables a robot to autonomously reach a goal state, such as a specific position and orientation relative to a specific map. Given a current pose, a map, and a goal, such as a destination pose, the navigation system generates a plan to reach the goal, and outputs commands to autonomously drive the robot, respecting any safety constraints and avoiding obstacles encountered along the way.","title":"Overview"},{"location":"install_instruction/","text":"Intel\u00ae Robot SDK Intel\u00ae Robot SDK is the tool to generate Robotics Software Development Kit (SDK) designed for autonomous devices, including the ROS2 core and capabilities packages like perception, planning, control driver etc. It provides flexible build and runtime configurations to enable several different use cases to be allocated to chosen hardware components. For example, one can use the GPU or VPU to accelerate CNN models. After build, the SDK is installed on Ubuntu18.04 with below items on development machine for further development. ROS2 core ROS2 Object Analytics(OA) with RealSense\u2122 and Movidius NCS2 ROS2 OpenVINO for people detection Gazebo 9 with Gazebo-ros2 simulator ROS2 navigation rviz2 ROS2 tutorial for Intel\u00ae components Robot support Turtlebot 3 Waffle Hardware Requirements An x86_64 computer running Ubuntu 18.04 or ADLink Neuron Board Intel\u00ae Movidius Neural Compute Stick 2 Intel\u00ae RealSense\u2122 D400 Series Software Requirements We support Ubuntu Linux 18.04(Bionic Beaver) on 64-bit. We do not support Mac OS X and Windows. Robot SDK Installation Steps Automated installation git clone https://github.com/intel/robot_sdk.git cd robot_sdk ./products/tb3/tb3_install.sh Manual installation # Build ROS2 packages ./robot_sdk.sh product tb3 # select tb3 as target product ./robot_sdk.sh sync-src all --force # sync all ROS2 source code to sdk_ws folder, clean folder with \"--force\" ./robot_sdk.sh build all --include-deps --cmake-args -DCMAKE_BUILD_TYPE=Release # build all ROS2 packges and build dependences with \"--include-deps\" ./robot_sdk.sh install # install generated ROS2 workspace to /opt/robot_sdk/ folder # Remove ROS2 packages ./robot_sdk.sh clean # remove build folders. ./robot_sdk.sh uninstall # uninstall generated ROS2 workspace from /opt/robot_sdk and delete sdk_ws build folder The build workspace overlay will be like this: . \u2514\u2500\u2500 sdk_ws \u251c\u2500\u2500 core_ws # ROS2 core packages from https://github.com/intel/robot_sdk/tree/master/products/tb3/core/ros2.repos. \u2502 \u251c\u2500\u2500 build \u2502 \u251c\u2500\u2500 install \u2502 \u2514\u2500\u2500 src \u251c\u2500\u2500 device_ws # Turtlebot3 packages from https://github.com/intel/robot_sdk/tree/master/products/tb3/device/repos/turtlebot3.repos. \u2502 \u251c\u2500\u2500 build \u2502 \u251c\u2500\u2500 install \u2502 \u2514\u2500\u2500 src \u2514\u2500\u2500 modules_ws # Intel\u00ae packages from https://github.com/intel/robot_sdk/tree/master/products/tb3/repos/intel.repos. \u251c\u2500\u2500 build \u251c\u2500\u2500 install \u2514\u2500\u2500 src Develop ROS 2 packages After Robot SDK build environment installed, you could source setup.bash and develop ROS 2 package. Edit installed component (navigation2 for example) cd ~/robot_sdk source /opt/robot_sdk/robot_sdk_setup.bash cd sdk_ws/device_ws colcon build --symlink-install --base-paths src/navigation2 source install/local_setup.bash Create new project #First source the robot sdk setup, then create a workspace for your new project, for example: source /opt/robot_sdk/robot_sdk_setup.bash mkdir ~/ros2_ws #Then add your repo to the workspace: mkdir ~/ros2/src/my_repo Using colcon to build a custom package. Report Issue If run into any issue of SDK, feel free to report issue in this project. Known Issues Multi-robot will impact each other while run in same network, ROS2 recommends to use \"domain ID\" but in this system, it might break the communication between NUC and OpenCR board. Make sure \"domain ID\" should be unset in TB3 environment. Issue reported to eProsima/Micro-XRCE-DDS-Agent/issues/66 . Turtlebot3 not work while Micro-XRCE-DDS-Agent build twice. Issue reported to eProsima/Micro-XRCE-DDS-Agent/issues/70 .","title":"Install Instruction"},{"location":"license/","text":"Copyright 2018 Intel\u00ae Corporation Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this project except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. Other names and brands may be claimed as the property of others. Any security issue should be reported using process at https://01.org/security.","title":"License"},{"location":"lidar_slam/","text":"1. Overview Robot SDK has integrated Cartographer for SLAM. For details, please refer to here . 2. Running the demo Terminal 1: Run Micro-XRCE-DDS Agent for OpenCR cd ~/turtlebot3 && MicroXRCEAgent serial /dev/ttyACM0 Terminal 2: Run Micro-XRCE-DDS Agent for Lidar cd ~/turtlebot3 && MicroXRCEAgent udp 2018 Terminal 3: Run Lidar application ~/turtlebot3/turtlebot3_lidar Terminal 4: Launch robot turtlebot3_node source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=waffle ros2 launch turtlebot3_bringup robot.launch.py Terminal 5: Run teleoperation node source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=waffle ros2 run turtlebot3_teleop teleop_keyboard Terminal 6: Run cartographer source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=waffle ros2 launch turtlebot3_cartographer cartographer.launch.py Terminal 7: Save the map source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=waffle ros2 run nav2_map_server map_saver -f ~/map","title":"SLAM with Lidar"},{"location":"lidar_slam/#1-overview","text":"Robot SDK has integrated Cartographer for SLAM. For details, please refer to here .","title":"1. Overview"},{"location":"lidar_slam/#2-running-the-demo","text":"Terminal 1: Run Micro-XRCE-DDS Agent for OpenCR cd ~/turtlebot3 && MicroXRCEAgent serial /dev/ttyACM0 Terminal 2: Run Micro-XRCE-DDS Agent for Lidar cd ~/turtlebot3 && MicroXRCEAgent udp 2018 Terminal 3: Run Lidar application ~/turtlebot3/turtlebot3_lidar Terminal 4: Launch robot turtlebot3_node source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=waffle ros2 launch turtlebot3_bringup robot.launch.py Terminal 5: Run teleoperation node source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=waffle ros2 run turtlebot3_teleop teleop_keyboard Terminal 6: Run cartographer source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=waffle ros2 launch turtlebot3_cartographer cartographer.launch.py Terminal 7: Save the map source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=waffle ros2 run nav2_map_server map_saver -f ~/map","title":" 2. Running the demo"},{"location":"navigation2/","text":"1. Overview The ROS 2 Navigation System is the control system that enables a robot to autonomously reach a goal state, such as a specific position and orientation relative to a specific map. Given a current pose, a map, and a goal, such as a destination pose, the navigation system generates a plan to reach the goal, and outputs commands to autonomously drive the robot, respecting any safety constraints and avoiding obstacles encountered along the way. 2. Running the demo The nav2_bringup package is an example bringup system for navigation2 applications. Notes: * OS Requirements: Ubuntu 18.04 * It is recommended to start with simulation using Gazebo before proceeding to run on a physical robot 2.1 Launch Navigation2 with Turtlebot3 in Gazebo simulator Terminal 1: Launch Gazebo and Rviz2 Example: See turtlebot3_gazebo models for details source /opt/robot_sdk/robot_sdk_setup.bash export GAZEBO_MODEL_PATH=$GAZEBO_MODEL_PATH:~/robot_sdk/sdk_ws/device_ws/src/turtlebot3/turtlebot3_simulations/turtlebot3_gazebo/models/ ros2 launch nav2_bringup gazebo_rviz2_launch.py world:=$HOME/robot_sdk/sdk_ws/device_ws/src/navigation2/navigation2/nav2_system_tests/words/turtlebot3_ros2_demo.world Terminal 2: Launch Turtlebot3 transforms source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=burger ros2 launch turtlebot3_bringup turtlebot3_state_publisher.launch.py Example: See turtlebot3_gazebo for details Terminal 3: Launch map_server and AMCL source /opt/robot_sdk/robot_sdk_setup.bash # Set the tf publisher node to use simulation time or AMCL won't get the transforms correctly ros2 param set /robot_state_publisher use_sim_time True # Launch map_server and AMCL, set map_type as \"occupancy\" by default. ros2 launch nav2_bringup nav2_bringup_1st_launch.py map:=$HOME/robot_sdk/sdk_ws/device_ws/src/navigation2/navigation2/nav2_system_tests/maps/map_circular.pgm map_type:=occupancy use_sim_time:=True # TODO: move the map to somewhere else. In RVIZ2: * Make sure all transforms from odom are present. (odom->base_link->base_scan) * Localize the robot using \u201c2D Pose Estimate\u201d button. Terminal 4: Run the rest of the Navigation2 bringup source /opt/robot_sdk/robot_sdk_setup.bash ros2 launch nav2_bringup nav2_bringup_2nd_launch.py use_sim_time:=True Terminal 5: Set the World Model and the two costmap nodes to use simulation time. source /opt/robot_sdk/robot_sdk_setup.bash ros2 param set /world_model use_sim_time True ros2 param set /global_costmap/global_costmap use_sim_time True ros2 param set /local_costmap/local_costmap use_sim_time True Notes: * The robot should be localized using the \u201c2D Pose Estimate\u201d button in Rviz2 before enabling use_sim_time . Otherwise, the ros2 param set commands on the costmaps will hang until that is done. * Setting use_sim_time has to be done dynamically after the nodes are up due to this bug:https://github.com/ros2/rclcpp/issues/595. * Sim time needs to be set in every namespace individually. * Sometimes setting use_sim_time a second time is required for all the nodes to get updated. * IF you continue to see WARN messages like the ones below, retry setting the use_sim_time parameter. [WARN] [world_model]: Costmap2DROS transform timeout. Current time: 1543616767.1026, global_pose stamp: 758.8040, tolerance: 0.3000, difference: 1543616008.2986 [WARN] [FollowPathNode]: Costmap2DROS transform timeout. Current time: 1543616767.2787, global_pose stamp: 759.0040, tolerance: 0.3000, difference: 1543616008.2747 In RVIZ2: * Add \"map\" to subscribe topic \"/map\". * Localize the robot using \"2D Pose Estimate\" button. * Send the robot a goal using \"2D Nav Goal\" button. * Make sure all transforms from odom are present. (odom->base_link->base_scan) 2.2 Launch Navigation2 on a Robot Terminal 1: Run Micro-XRCE-DDS Agent for OpenCR cd ~/turtlebot3 && MicroXRCEAgent serial /dev/ttyACM0 Terminal 2: Run Micro-XRCE-DDS Agent for Lidar cd ~/turtlebot3 && MicroXRCEAgent udp 2018 Terminal 3: Run Lidar application ~/turtlebot3/turtlebot3_lidar Terminal 4: Launch robot turtlebot3_node source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=waffle ros2 launch turtlebot3_bringup robot.launch.py Terminal 5: Run AMCL and Map Server source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=waffle ros2 launch nav2_bringup nav2_bringup_1st_launch.py map:=/full/path/to/map.yaml Terminal 6: Run RVIZ2 ros2 run rviz2 rviz2 In RVIZ2: Add components: map, scan, tf, global_costmap, local_costmap Make sure all transforms from odom are present. Localize the robot using 2D Pose Estimate button. Terminal 7: Run the rest of the Navigation2 bringup source /opt/robot_sdk/robot_sdk_setup.bash ros2 launch nav2_bringup nav2_bringup_2nd_launch.py In RVIZ2: Send the robot a goal using 2D Nav Goal button. If nothing happens, first retry localizing the robot using the 2D Pose Estimate button, then retry sending the goal using the 2D Nav Goal button. 3. Known issues * This stack and ROS2 are still in heavy development and there are some bugs and stability issues being worked on, so please do not try this on a robot without taking heavy safety precautions. THE ROBOT MAY CRASH! * For a current list of known issues, see https://github.com/ros-planning/navigation2/issues . 4. ToDo Add additional maps and examples.","title":"Navigation2"},{"location":"navigation2/#1-overview","text":"The ROS 2 Navigation System is the control system that enables a robot to autonomously reach a goal state, such as a specific position and orientation relative to a specific map. Given a current pose, a map, and a goal, such as a destination pose, the navigation system generates a plan to reach the goal, and outputs commands to autonomously drive the robot, respecting any safety constraints and avoiding obstacles encountered along the way.","title":"1. Overview"},{"location":"navigation2/#2-running-the-demo","text":"The nav2_bringup package is an example bringup system for navigation2 applications. Notes: * OS Requirements: Ubuntu 18.04 * It is recommended to start with simulation using Gazebo before proceeding to run on a physical robot","title":"2. Running the demo"},{"location":"navigation2/#21-launch-navigation2-with-turtlebot3-in-gazebo-simulator","text":"","title":"2.1 Launch Navigation2 with Turtlebot3 in Gazebo simulator"},{"location":"navigation2/#terminal-1-launch-gazebo-and-rviz2","text":"Example: See turtlebot3_gazebo models for details source /opt/robot_sdk/robot_sdk_setup.bash export GAZEBO_MODEL_PATH=$GAZEBO_MODEL_PATH:~/robot_sdk/sdk_ws/device_ws/src/turtlebot3/turtlebot3_simulations/turtlebot3_gazebo/models/ ros2 launch nav2_bringup gazebo_rviz2_launch.py world:=$HOME/robot_sdk/sdk_ws/device_ws/src/navigation2/navigation2/nav2_system_tests/words/turtlebot3_ros2_demo.world","title":"Terminal 1: Launch Gazebo and Rviz2"},{"location":"navigation2/#terminal-2-launch-turtlebot3-transforms","text":"source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=burger ros2 launch turtlebot3_bringup turtlebot3_state_publisher.launch.py Example: See turtlebot3_gazebo for details","title":"Terminal 2: Launch Turtlebot3 transforms"},{"location":"navigation2/#terminal-3-launch-map_server-and-amcl","text":"source /opt/robot_sdk/robot_sdk_setup.bash # Set the tf publisher node to use simulation time or AMCL won't get the transforms correctly ros2 param set /robot_state_publisher use_sim_time True # Launch map_server and AMCL, set map_type as \"occupancy\" by default. ros2 launch nav2_bringup nav2_bringup_1st_launch.py map:=$HOME/robot_sdk/sdk_ws/device_ws/src/navigation2/navigation2/nav2_system_tests/maps/map_circular.pgm map_type:=occupancy use_sim_time:=True # TODO: move the map to somewhere else. In RVIZ2: * Make sure all transforms from odom are present. (odom->base_link->base_scan) * Localize the robot using \u201c2D Pose Estimate\u201d button.","title":"Terminal 3: Launch map_server and AMCL"},{"location":"navigation2/#terminal-4","text":"Run the rest of the Navigation2 bringup source /opt/robot_sdk/robot_sdk_setup.bash ros2 launch nav2_bringup nav2_bringup_2nd_launch.py use_sim_time:=True","title":"Terminal 4:"},{"location":"navigation2/#terminal-5","text":"Set the World Model and the two costmap nodes to use simulation time. source /opt/robot_sdk/robot_sdk_setup.bash ros2 param set /world_model use_sim_time True ros2 param set /global_costmap/global_costmap use_sim_time True ros2 param set /local_costmap/local_costmap use_sim_time True Notes: * The robot should be localized using the \u201c2D Pose Estimate\u201d button in Rviz2 before enabling use_sim_time . Otherwise, the ros2 param set commands on the costmaps will hang until that is done. * Setting use_sim_time has to be done dynamically after the nodes are up due to this bug:https://github.com/ros2/rclcpp/issues/595. * Sim time needs to be set in every namespace individually. * Sometimes setting use_sim_time a second time is required for all the nodes to get updated. * IF you continue to see WARN messages like the ones below, retry setting the use_sim_time parameter. [WARN] [world_model]: Costmap2DROS transform timeout. Current time: 1543616767.1026, global_pose stamp: 758.8040, tolerance: 0.3000, difference: 1543616008.2986 [WARN] [FollowPathNode]: Costmap2DROS transform timeout. Current time: 1543616767.2787, global_pose stamp: 759.0040, tolerance: 0.3000, difference: 1543616008.2747 In RVIZ2: * Add \"map\" to subscribe topic \"/map\". * Localize the robot using \"2D Pose Estimate\" button. * Send the robot a goal using \"2D Nav Goal\" button. * Make sure all transforms from odom are present. (odom->base_link->base_scan)","title":"Terminal 5:"},{"location":"navigation2/#22-launch-navigation2-on-a-robot","text":"","title":"2.2 Launch Navigation2 on a Robot"},{"location":"navigation2/#terminal-1-run-micro-xrce-dds-agent-for-opencr","text":"cd ~/turtlebot3 && MicroXRCEAgent serial /dev/ttyACM0","title":"Terminal 1: Run Micro-XRCE-DDS Agent for OpenCR"},{"location":"navigation2/#terminal-2-run-micro-xrce-dds-agent-for-lidar","text":"cd ~/turtlebot3 && MicroXRCEAgent udp 2018","title":"Terminal 2: Run Micro-XRCE-DDS Agent for Lidar"},{"location":"navigation2/#terminal-3-run-lidar-application","text":"~/turtlebot3/turtlebot3_lidar","title":"Terminal 3: Run Lidar application"},{"location":"navigation2/#terminal-4-launch-robot-turtlebot3_node","text":"source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=waffle ros2 launch turtlebot3_bringup robot.launch.py","title":"Terminal 4: Launch robot turtlebot3_node"},{"location":"navigation2/#terminal-5-run-amcl-and-map-server","text":"source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=waffle ros2 launch nav2_bringup nav2_bringup_1st_launch.py map:=/full/path/to/map.yaml","title":"Terminal 5: Run AMCL and Map Server"},{"location":"navigation2/#terminal-6-run-rviz2","text":"ros2 run rviz2 rviz2 In RVIZ2: Add components: map, scan, tf, global_costmap, local_costmap Make sure all transforms from odom are present. Localize the robot using 2D Pose Estimate button.","title":"Terminal 6: Run RVIZ2"},{"location":"navigation2/#terminal-7-run-the-rest-of-the-navigation2-bringup","text":"source /opt/robot_sdk/robot_sdk_setup.bash ros2 launch nav2_bringup nav2_bringup_2nd_launch.py In RVIZ2: Send the robot a goal using 2D Nav Goal button. If nothing happens, first retry localizing the robot using the 2D Pose Estimate button, then retry sending the goal using the 2D Nav Goal button.","title":"Terminal 7: Run the rest of the Navigation2 bringup"},{"location":"navigation2/#3-known-issues","text":"* This stack and ROS2 are still in heavy development and there are some bugs and stability issues being worked on, so please do not try this on a robot without taking heavy safety precautions. THE ROBOT MAY CRASH! * For a current list of known issues, see https://github.com/ros-planning/navigation2/issues .","title":"3. Known issues"},{"location":"navigation2/#4-todo","text":"Add additional maps and examples.","title":"4. ToDo"},{"location":"oa/","text":"1. Overview Object Analytics (OA) is ROS2 wrapper for realtime object detection, localization and tracking. These packages aim to provide real-time object analyses over RGB-D camera inputs, enabling ROS developers to easily create amazing robotics advanced features, like intelligent collision avoidance and semantic SLAM. It consumes sensor_msgs::PointClould2 data delivered by RGB-D camera, publishing topics on object detection , object tracking , and object localization in 3D camera coordination system. OA keeps integrating with various \"state-of-the-art\" algorithms. By default, backend of object detection is Intel\u00ae movidius ncs2. 2. Running the demo Object Analytics with OpenVINO toolkit # Start OA demo with OpenVINO source /opt/robot_sdk/robot_sdk_setup.bash ros2 launch object_analytics_node object_analytics_with_openvino_sdk.launch.py Customize launch By default, object analytics will launch both tracking and localization features, but either tracking or localization or both can be dropped. Detailed please refer comments embedded in launch file. 3. Interfaces Subscribed topics * /object_analytics/detected_objects ( object_msgs::msg::ObjectsInBoxes ) Published topics * /object_analytics/rgb ( sensor_msgs::msg::Image ) * /object_analytics/pointcloud ( sensor_msgs::msg::PointCloud2 ) * /object_analytics/localization ( object_analytics_msgs::msg::ObjectsInBoxes3D ) * /object_analytics/tracking ( object_analytics_msgs::msg::TrackedObjects ) * /object_analytics/movement ( object_analytics_msgs::msg::MovingObjectsInFrame ) 4. Known issues -- 5. ToDo --","title":"Object Analytics"},{"location":"oa/#1-overview","text":"Object Analytics (OA) is ROS2 wrapper for realtime object detection, localization and tracking. These packages aim to provide real-time object analyses over RGB-D camera inputs, enabling ROS developers to easily create amazing robotics advanced features, like intelligent collision avoidance and semantic SLAM. It consumes sensor_msgs::PointClould2 data delivered by RGB-D camera, publishing topics on object detection , object tracking , and object localization in 3D camera coordination system. OA keeps integrating with various \"state-of-the-art\" algorithms. By default, backend of object detection is Intel\u00ae movidius ncs2.","title":"1. Overview"},{"location":"oa/#2-running-the-demo","text":"Object Analytics with OpenVINO toolkit # Start OA demo with OpenVINO source /opt/robot_sdk/robot_sdk_setup.bash ros2 launch object_analytics_node object_analytics_with_openvino_sdk.launch.py Customize launch By default, object analytics will launch both tracking and localization features, but either tracking or localization or both can be dropped. Detailed please refer comments embedded in launch file.","title":"2. Running the demo"},{"location":"oa/#3-interfaces","text":"Subscribed topics * /object_analytics/detected_objects ( object_msgs::msg::ObjectsInBoxes ) Published topics * /object_analytics/rgb ( sensor_msgs::msg::Image ) * /object_analytics/pointcloud ( sensor_msgs::msg::PointCloud2 ) * /object_analytics/localization ( object_analytics_msgs::msg::ObjectsInBoxes3D ) * /object_analytics/tracking ( object_analytics_msgs::msg::TrackedObjects ) * /object_analytics/movement ( object_analytics_msgs::msg::MovingObjectsInFrame )","title":"3. Interfaces"},{"location":"oa/#4-known-issues","text":"--","title":"4. Known issues"},{"location":"oa/#5-todo","text":"--","title":"5. ToDo"},{"location":"realsense/","text":"1. Overview These are packages for using Intel\u00ae RealSense\u2122 cameras (D400 series) with ROS2. 2. Running the demo Start the camera node To start the camera node in ROS2, plug in the camera, then type the following command: source /opt/robot_sdk/robot_sdk_setup.bash # To launch with \"ros2 run\" ros2 run realsense_ros2_camera realsense_ros2_camera # OR, to invoke the executable directly realsense_ros2_camera This will stream all camera sensors and publish on the appropriate ROS2 topics. PointCloud2 is enabled by default. View camera data # in Terminal #1 launch realsense_ros2_camera source /opt/robot_sdk/robot_sdk_setup.bash realsense_ros2_camera # in terminal #2 launch rviz2 source /opt/robot_sdk/robot_sdk_setup.bash rviz2 This will launch RViz2 and display the five streams: color, depth, infra1, infra2, pointcloud. Realsense can also support SLAM and navigation, see here . 3. Key Interfaces * /camera/depth/image_rect_raw * /camera/color/image_raw * /camera/infra1/image_rect_raw * /camera/infra2/image_rect_raw * /camera/depth/color/points 4. Known Issues * This ROS2 node does not currently provide any dynamic reconfigure support for camera properties/presets. * We support Ubuntu Linux 18.04 (Bionic Beaver) on 64-bit, but not support Mac OS X and Windows yet. 5. ToDo A few features to be ported from the latest realsense_ros_camera v2.0.2 * RGB-D point cloud (depth_registered) * Preset/Controls","title":"Intel Realsense"},{"location":"realsense/#1-overview","text":"These are packages for using Intel\u00ae RealSense\u2122 cameras (D400 series) with ROS2.","title":"1. Overview"},{"location":"realsense/#2-running-the-demo","text":"Start the camera node To start the camera node in ROS2, plug in the camera, then type the following command: source /opt/robot_sdk/robot_sdk_setup.bash # To launch with \"ros2 run\" ros2 run realsense_ros2_camera realsense_ros2_camera # OR, to invoke the executable directly realsense_ros2_camera This will stream all camera sensors and publish on the appropriate ROS2 topics. PointCloud2 is enabled by default. View camera data # in Terminal #1 launch realsense_ros2_camera source /opt/robot_sdk/robot_sdk_setup.bash realsense_ros2_camera # in terminal #2 launch rviz2 source /opt/robot_sdk/robot_sdk_setup.bash rviz2 This will launch RViz2 and display the five streams: color, depth, infra1, infra2, pointcloud. Realsense can also support SLAM and navigation, see here .","title":"2. Running the demo"},{"location":"realsense/#3-key-interfaces","text":"* /camera/depth/image_rect_raw * /camera/color/image_raw * /camera/infra1/image_rect_raw * /camera/infra2/image_rect_raw * /camera/depth/color/points","title":"3. Key Interfaces"},{"location":"realsense/#4-known-issues","text":"* This ROS2 node does not currently provide any dynamic reconfigure support for camera properties/presets. * We support Ubuntu Linux 18.04 (Bionic Beaver) on 64-bit, but not support Mac OS X and Windows yet.","title":"4. Known Issues"},{"location":"realsense/#5-todo","text":"A few features to be ported from the latest realsense_ros_camera v2.0.2 * RGB-D point cloud (depth_registered) * Preset/Controls","title":"5. ToDo"},{"location":"rs_for_slam_nav/","text":"1. Overview SLAM with cartographer requires laser scan data for robot pose estimation. Intel\u00ae RealSense\u2122 depth cameras can generate depth image, which can be converted to laser scan with depthimage_to_laserscan package, therefore, we provide a way to use RealSense\u2122 for SLAM and navigation. 2. SLAM with RealSense\u2122 Disable the laser scan from turtlebot3 LDS By default, turtlebot3 cartographer uses the laser scan from the inherent 360 LDS, so it's necessary to disable that first. vim ~/robot_sdk/sdk_ws/device_ws/src/turtlebot3/turtlebot3/turtlebot3_node/src/node_main.cpp please comment out the following codes: line 57 laser_scan_pub_ = this->create_publisher<sensor_msgs::msg::LaserScan>(ScanTopic, rmw_qos_profile_default); line 69 - 75 auto laser_scan_callback = [this](const sensor_msgs::msg::LaserScan::SharedPtr laser_scan) -> void { this->lidar_->makeFullRange(laser_scan); }; laser_scan_sub_ = this->create_subscription<sensor_msgs::msg::LaserScan>(ScanHalfTopic, laser_scan_callback); line 93 - 99 laser_scan_timer_ = this->create_wall_timer( ScanPublishPeriodMillis, [this]() { this->laser_scan_pub_->publish(this->lidar_->getLaserScan(this->now())); } ); line 134 rclcpp::Subscription<sensor_msgs::msg::LaserScan>::SharedPtr laser_scan_sub_; line 139 rclcpp::Publisher<sensor_msgs::msg::LaserScan>::SharedPtr laser_scan_pub_; Next, it needs to re-build the turtlebot3_node to enable the modification source /opt/robot_sdk/robot_sdk_setup.bash cd ~/robot_sdk/sdk_ws/device_ws colcon build --packages-select turtlebot3_node Adapt the turtlebot3 robot model The example here is based on the turtlebot3 model of waffle. If another RealSense\u2122 RGB-D camera(e.g D435) is used, please remove the original R200 camera and mount the new RealSense\u2122 camera at the same place of R200 on the turtlebot3 waffle chassis and override the following waffle model description source /opt/robot_sdk/robot_sdk_setup.bash cp -v ~/robot_sdk/products/tb3/device/urdf/turtlebot3_waffle.urdf.xacro <the-path-of-turtlebot3>/turtlebot3_description/urdf/turtlebot3_waffle.urdf colcon build --symlink-install --packages-select turtlebot3_description Resolve the package conflict of FastRTPS Turtlebot3 depends on Micro-DDS which requires to build with a dedicated commit from FastRTPS, however, the ROS2 core has already built a FastRTPS package, it results in the laser scan derived from RealSense\u2122 depth gotten lost if those two different FastRTPS used. So it needs to work around them into the same FastRTPS package, which is the one from ros2 core. Please change the code with the following steps: # modify the Micro-XRCE-DDS-Agent's source code to build with the ROS@ FastRTPS package vim ~/Micro-XRCE-DDS-Agent/src/cpp/Root.cpp change the line 34-46 to Root::Root() : mtx_(), clients_(), current_client_() { current_client_ = clients_.begin(); /* Load XML profile file. */ fastrtps::xmlparser::XMLProfileManager::loadDefaultXMLFile(); } and re-build the package cd ~/Micro-XRCE-DDS-Agent/build cmake .. sudo make install Convert RealSense\u2122 depth to laser scan source /opt/robot_sdk/robot_sdk_setup.bash git clone https://github.com/ros-perception/depthimage_to_laserscan.git -b ros2 colcon build --symlink-install --packages-select depthimage_to_laserscan ros2 launch realsense_ros2_camera realsense2_to_laserscan.py Tune the cartographer configuration At present, high ratio sampling rate of odometry drifts the map building, please change it accordingly. vim <the-path-of-turtlebot3>/turtlebot3_cartographer/config/turtlebot3_lds_2d.lua change the line 41 to odometry_sampling_ratio = 0.5, and re-build the package turtlebot3_cartographer if it's already been built before. Start to SLAM # In terminal 1 cd ~/turtlebot3 && MicroXRCEAgent serial /dev/ttyACM0 # In terminal 2 source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=waffle && ros2 launch turtlebot3_bringup robot.launch.py # In terminal 3 source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=waffle && ros2 run turtlebot3_teleop teleop_keyboard # In terminal 4 source /opt/robot_sdk/robot_sdk_setup.bash ros2 launch turtlebot3_cartographer cartographer.launch.py Control and move the turtlebot3 with keyboard to build map, and when the map building process is done, please save the map with the following command: # In terminal 5 source /opt/robot_sdk/robot_sdk_setup.bash ros2 run nav2_map_server map_saver -f ~/map Next, try to open and preview the map.pgm to confirm it. The following is a map built with RealSense\u2122 and cartographer: Additionally, the one with LDS is as follows: 3. Navigation with RealSense\u2122 Generally, In order to navigation with the map from SLAM with RealSense\u2122, the ros2 navigation stack should be built and ready to use. Bringup the turtlebot3 # In terminal 1 cd ~/turtlebot3 && MicroXRCEAgent serial /dev/ttyACM0 # In terminal 2 source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=waffle && ros2 launch turtlebot3_bringup robot.launch.py Start ROS2 realsense and depth image to laser scan # In terminal 3 source /opt/robot_sdk/robot_sdk_setup.bash ros2 launch realsense_ros2_camera realsense2_to_laserscan.py Read and distribute map with map server # In terminal 4 source /opt/robot_sdk/robot_sdk_setup.bash ros2 run nav2_map_server map_server -f ~/map Start the navigation2 stack with the map # In terminal 5 export TURTLEBOT3_MODEL=waffle ros2 launch turtlebot3_navigation2 navigation2.launch.py map:=${HOME}/map.yaml Finally, please give an initial pose and goal within RVIZ2 to direct and navigate the turtlebot3 with the running map. 4. Known issues * The accuracy of RealSense\u2122 depth depends on the detection distance and the quality may not good enough to build a big map, it drifts the map building. * Keep the RealSense\u2122 parallel to the ground, or the tilt of the RealSense\u2122 may influence the SLAM.","title":"Realsense for SLAM and Navigation"},{"location":"rs_for_slam_nav/#1-overview","text":"SLAM with cartographer requires laser scan data for robot pose estimation. Intel\u00ae RealSense\u2122 depth cameras can generate depth image, which can be converted to laser scan with depthimage_to_laserscan package, therefore, we provide a way to use RealSense\u2122 for SLAM and navigation.","title":"1. Overview"},{"location":"rs_for_slam_nav/#2-slam-with-realsensetm","text":"Disable the laser scan from turtlebot3 LDS By default, turtlebot3 cartographer uses the laser scan from the inherent 360 LDS, so it's necessary to disable that first. vim ~/robot_sdk/sdk_ws/device_ws/src/turtlebot3/turtlebot3/turtlebot3_node/src/node_main.cpp please comment out the following codes: line 57 laser_scan_pub_ = this->create_publisher<sensor_msgs::msg::LaserScan>(ScanTopic, rmw_qos_profile_default); line 69 - 75 auto laser_scan_callback = [this](const sensor_msgs::msg::LaserScan::SharedPtr laser_scan) -> void { this->lidar_->makeFullRange(laser_scan); }; laser_scan_sub_ = this->create_subscription<sensor_msgs::msg::LaserScan>(ScanHalfTopic, laser_scan_callback); line 93 - 99 laser_scan_timer_ = this->create_wall_timer( ScanPublishPeriodMillis, [this]() { this->laser_scan_pub_->publish(this->lidar_->getLaserScan(this->now())); } ); line 134 rclcpp::Subscription<sensor_msgs::msg::LaserScan>::SharedPtr laser_scan_sub_; line 139 rclcpp::Publisher<sensor_msgs::msg::LaserScan>::SharedPtr laser_scan_pub_; Next, it needs to re-build the turtlebot3_node to enable the modification source /opt/robot_sdk/robot_sdk_setup.bash cd ~/robot_sdk/sdk_ws/device_ws colcon build --packages-select turtlebot3_node Adapt the turtlebot3 robot model The example here is based on the turtlebot3 model of waffle. If another RealSense\u2122 RGB-D camera(e.g D435) is used, please remove the original R200 camera and mount the new RealSense\u2122 camera at the same place of R200 on the turtlebot3 waffle chassis and override the following waffle model description source /opt/robot_sdk/robot_sdk_setup.bash cp -v ~/robot_sdk/products/tb3/device/urdf/turtlebot3_waffle.urdf.xacro <the-path-of-turtlebot3>/turtlebot3_description/urdf/turtlebot3_waffle.urdf colcon build --symlink-install --packages-select turtlebot3_description Resolve the package conflict of FastRTPS Turtlebot3 depends on Micro-DDS which requires to build with a dedicated commit from FastRTPS, however, the ROS2 core has already built a FastRTPS package, it results in the laser scan derived from RealSense\u2122 depth gotten lost if those two different FastRTPS used. So it needs to work around them into the same FastRTPS package, which is the one from ros2 core. Please change the code with the following steps: # modify the Micro-XRCE-DDS-Agent's source code to build with the ROS@ FastRTPS package vim ~/Micro-XRCE-DDS-Agent/src/cpp/Root.cpp change the line 34-46 to Root::Root() : mtx_(), clients_(), current_client_() { current_client_ = clients_.begin(); /* Load XML profile file. */ fastrtps::xmlparser::XMLProfileManager::loadDefaultXMLFile(); } and re-build the package cd ~/Micro-XRCE-DDS-Agent/build cmake .. sudo make install Convert RealSense\u2122 depth to laser scan source /opt/robot_sdk/robot_sdk_setup.bash git clone https://github.com/ros-perception/depthimage_to_laserscan.git -b ros2 colcon build --symlink-install --packages-select depthimage_to_laserscan ros2 launch realsense_ros2_camera realsense2_to_laserscan.py Tune the cartographer configuration At present, high ratio sampling rate of odometry drifts the map building, please change it accordingly. vim <the-path-of-turtlebot3>/turtlebot3_cartographer/config/turtlebot3_lds_2d.lua change the line 41 to odometry_sampling_ratio = 0.5, and re-build the package turtlebot3_cartographer if it's already been built before. Start to SLAM # In terminal 1 cd ~/turtlebot3 && MicroXRCEAgent serial /dev/ttyACM0 # In terminal 2 source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=waffle && ros2 launch turtlebot3_bringup robot.launch.py # In terminal 3 source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=waffle && ros2 run turtlebot3_teleop teleop_keyboard # In terminal 4 source /opt/robot_sdk/robot_sdk_setup.bash ros2 launch turtlebot3_cartographer cartographer.launch.py Control and move the turtlebot3 with keyboard to build map, and when the map building process is done, please save the map with the following command: # In terminal 5 source /opt/robot_sdk/robot_sdk_setup.bash ros2 run nav2_map_server map_saver -f ~/map Next, try to open and preview the map.pgm to confirm it. The following is a map built with RealSense\u2122 and cartographer: Additionally, the one with LDS is as follows:","title":"2. SLAM with RealSense\u2122"},{"location":"rs_for_slam_nav/#3-navigation-with-realsensetm","text":"Generally, In order to navigation with the map from SLAM with RealSense\u2122, the ros2 navigation stack should be built and ready to use. Bringup the turtlebot3 # In terminal 1 cd ~/turtlebot3 && MicroXRCEAgent serial /dev/ttyACM0 # In terminal 2 source /opt/robot_sdk/robot_sdk_setup.bash export TURTLEBOT3_MODEL=waffle && ros2 launch turtlebot3_bringup robot.launch.py Start ROS2 realsense and depth image to laser scan # In terminal 3 source /opt/robot_sdk/robot_sdk_setup.bash ros2 launch realsense_ros2_camera realsense2_to_laserscan.py Read and distribute map with map server # In terminal 4 source /opt/robot_sdk/robot_sdk_setup.bash ros2 run nav2_map_server map_server -f ~/map Start the navigation2 stack with the map # In terminal 5 export TURTLEBOT3_MODEL=waffle ros2 launch turtlebot3_navigation2 navigation2.launch.py map:=${HOME}/map.yaml Finally, please give an initial pose and goal within RVIZ2 to direct and navigate the turtlebot3 with the running map.","title":"3. Navigation with RealSense\u2122"},{"location":"rs_for_slam_nav/#4-known-issues","text":"* The accuracy of RealSense\u2122 depth depends on the detection distance and the quality may not good enough to build a big map, it drifts the map building. * Keep the RealSense\u2122 parallel to the ground, or the tilt of the RealSense\u2122 may influence the SLAM.","title":"4. Known issues"},{"location":"vino/","text":"1. Overview The OpenVINO\u2122 (Open visual inference and neural network optimization) toolkit provides a ROS2 compatible runtime framework of neural network which quickly deploys applications and solutions for vision inference. By leveraging Intel\u00ae OpenVINO\u2122 toolkit and corresponding libraries, this runtime framework extends workloads across Intel\u00ae hardware (including accelerators) and maximizes performance. * Enables CNN-based deep learning inference at the edge. * Supports heterogeneous execution across computer vision accelerators\u2014CPU, GPU, Intel\u00ae Movidius\u2122 Neural Compute Stick, and FPGA\u2014using a common API. * Speeds up time to market via a library of functions and preoptimized kernels. * Includes optimized calls for OpenCV and OpenVX. 2. Running the demo * Preparation download and convert a trained model to produce an optimized Intermediate Representation (IR) of the model source /opt/intel/computer_vision_sdk/bin/setupvars.sh export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/intel/computer_vision_sdk/deployment_tools/inference_engine/samples/build/intel64/Release/lib * Connect Intel\u00ae Neural Compute Stick 2 to USB port of system \u2002\u2002 run face detection sample code input from standard RGB camera. source /opt/robot_sdk/robot_sdk_setup.bash ros2 launch dynamic_vino_sample pipeline_people_myriad.launch.py \u2002\u2002 run object detection sample code input from RealSense\u2122. source /opt/robot_sdk/robot_sdk_setup.bash ros2 launch dynamic_vino_sample pipeline_object.launch.py Please refer to here for more usages. 3. Key Interfaces Subscribed Topic * Image topic: /openvino_toolkit/image_raw( sensor_msgs::msg::Image ) Published Topic * Face Detection: /ros2_openvino_toolkit/face_detection( object_msgs::msg::ObjectsInBoxes ) * Emotion Recognition: /ros2_openvino_toolkit/emotions_recognition( people_msgs::msg::EmotionsStamped ) * Age and Gender Recognition: /ros2_openvino_toolkit/age_genders_Recognition( people_msgs::msg::AgeGenderStamped ) * Head Pose Estimation: /ros2_openvino_toolkit/headposes_estimation( people_msgs::msg::HeadPoseStamped ) * Object Detection: /ros2_openvino_toolkit/detected_objects( object_msgs::msg::ObjectsInBoxes ) * Object Segmentation: /ros2_openvino_toolkit/segmented_obejcts( people_msgs::msg::ObjectsInMasks ) * Rviz Output: /ros2_openvino_toolkit/image_rviz( sensor_msgs::msg::Image ) Service * Object Detection Service: /detect_object( object_msgs::srv::DetectObject ) * Face Detection Service: /detect_face( object_msgs::srv::DetectObject ) * Age & Gender Detection Service: /detect_age_gender( people_msgs::srv::AgeGender ) * Headpose Detection Service: /detect_head_pose( people_msgs::srv::HeadPose ) * Emotion Detection Service: /detect_emotion( people_msgs::srv::Emotion ) face detection input from image object detection input from RealSense\u2122 object segmentation input from video Person Reidentification input from standard camera 4. Known issues -- 5. ToDo * Support result filtering for inference process, so that the inference results can be filtered to different subsidiary inference. For example, given an image, firstly we do Object Detection on it, secondly we pass cars to vehicle brand recognition and pass license plate to license number recognition. * Design resource manager to better use such resources as models, engines, and other external plugins. * Develop GUI based configuration and management tools (and monitoring and diagnose tools), in order to provide easy entry for end users to simplify their operation.","title":"OpenVINO Tookit"},{"location":"vino/#1-overview","text":"The OpenVINO\u2122 (Open visual inference and neural network optimization) toolkit provides a ROS2 compatible runtime framework of neural network which quickly deploys applications and solutions for vision inference. By leveraging Intel\u00ae OpenVINO\u2122 toolkit and corresponding libraries, this runtime framework extends workloads across Intel\u00ae hardware (including accelerators) and maximizes performance. * Enables CNN-based deep learning inference at the edge. * Supports heterogeneous execution across computer vision accelerators\u2014CPU, GPU, Intel\u00ae Movidius\u2122 Neural Compute Stick, and FPGA\u2014using a common API. * Speeds up time to market via a library of functions and preoptimized kernels. * Includes optimized calls for OpenCV and OpenVX.","title":"1. Overview"},{"location":"vino/#2-running-the-demo","text":"* Preparation download and convert a trained model to produce an optimized Intermediate Representation (IR) of the model source /opt/intel/computer_vision_sdk/bin/setupvars.sh export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/intel/computer_vision_sdk/deployment_tools/inference_engine/samples/build/intel64/Release/lib * Connect Intel\u00ae Neural Compute Stick 2 to USB port of system \u2002\u2002 run face detection sample code input from standard RGB camera. source /opt/robot_sdk/robot_sdk_setup.bash ros2 launch dynamic_vino_sample pipeline_people_myriad.launch.py \u2002\u2002 run object detection sample code input from RealSense\u2122. source /opt/robot_sdk/robot_sdk_setup.bash ros2 launch dynamic_vino_sample pipeline_object.launch.py Please refer to here for more usages.","title":"2. Running the demo"},{"location":"vino/#3-key-interfaces","text":"Subscribed Topic * Image topic: /openvino_toolkit/image_raw( sensor_msgs::msg::Image ) Published Topic * Face Detection: /ros2_openvino_toolkit/face_detection( object_msgs::msg::ObjectsInBoxes ) * Emotion Recognition: /ros2_openvino_toolkit/emotions_recognition( people_msgs::msg::EmotionsStamped ) * Age and Gender Recognition: /ros2_openvino_toolkit/age_genders_Recognition( people_msgs::msg::AgeGenderStamped ) * Head Pose Estimation: /ros2_openvino_toolkit/headposes_estimation( people_msgs::msg::HeadPoseStamped ) * Object Detection: /ros2_openvino_toolkit/detected_objects( object_msgs::msg::ObjectsInBoxes ) * Object Segmentation: /ros2_openvino_toolkit/segmented_obejcts( people_msgs::msg::ObjectsInMasks ) * Rviz Output: /ros2_openvino_toolkit/image_rviz( sensor_msgs::msg::Image ) Service * Object Detection Service: /detect_object( object_msgs::srv::DetectObject ) * Face Detection Service: /detect_face( object_msgs::srv::DetectObject ) * Age & Gender Detection Service: /detect_age_gender( people_msgs::srv::AgeGender ) * Headpose Detection Service: /detect_head_pose( people_msgs::srv::HeadPose ) * Emotion Detection Service: /detect_emotion( people_msgs::srv::Emotion ) face detection input from image object detection input from RealSense\u2122 object segmentation input from video Person Reidentification input from standard camera","title":"3. Key Interfaces"},{"location":"vino/#4-known-issues","text":"--","title":"4. Known issues"},{"location":"vino/#5-todo","text":"* Support result filtering for inference process, so that the inference results can be filtered to different subsidiary inference. For example, given an image, firstly we do Object Detection on it, secondly we pass cars to vehicle brand recognition and pass license plate to license number recognition. * Design resource manager to better use such resources as models, engines, and other external plugins. * Develop GUI based configuration and management tools (and monitoring and diagnose tools), in order to provide easy entry for end users to simplify their operation.","title":"5. ToDo"}]}